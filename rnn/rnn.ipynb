{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('kafka.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = list(set(data))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'str'>\n",
      "Total characters : 138487 \n",
      "Unique Characters : 84\n"
     ]
    }
   ],
   "source": [
    "print(type(data))\n",
    "print(\"Total characters : %d \" % len(data))\n",
    "print(\"Unique Characters : %d\" % len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, 'C': 32, '!': 3, ' ': 4, '#': 5, '\"': 6, '%': 7, '$': 8, \"'\": 9, ')': 10, '(': 11, '*': 12, '-': 13, ',': 14, '/': 2, '.': 16, '1': 17, '0': 18, '3': 19, '2': 20, '5': 21, '4': 22, '7': 23, '6': 24, '9': 25, '8': 26, ';': 27, ':': 28, '?': 29, 'A': 30, '@': 31, '\\xc3': 1, 'B': 33, 'E': 34, 'D': 35, 'G': 36, 'F': 37, 'I': 38, 'H': 39, 'K': 40, 'J': 41, 'M': 42, 'L': 43, 'O': 44, 'N': 45, 'Q': 46, 'P': 47, 'S': 48, 'R': 49, 'U': 50, 'T': 51, 'W': 52, 'V': 53, 'Y': 54, 'X': 55, '[': 56, ']': 57, 'd': 62, 'a': 58, 'c': 59, 'b': 60, 'e': 61, '\\xa7': 15, 'g': 63, 'f': 64, 'i': 65, 'h': 66, 'k': 67, 'j': 68, 'm': 69, 'l': 70, 'o': 71, 'n': 72, 'q': 73, 'p': 74, 's': 75, 'r': 76, 'u': 77, 't': 78, 'w': 79, 'v': 80, 'y': 81, 'x': 82, 'z': 83}\n",
      "{0: '\\n', 1: '\\xc3', 2: '/', 3: '!', 4: ' ', 5: '#', 6: '\"', 7: '%', 8: '$', 9: \"'\", 10: ')', 11: '(', 12: '*', 13: '-', 14: ',', 15: '\\xa7', 16: '.', 17: '1', 18: '0', 19: '3', 20: '2', 21: '5', 22: '4', 23: '7', 24: '6', 25: '9', 26: '8', 27: ';', 28: ':', 29: '?', 30: 'A', 31: '@', 32: 'C', 33: 'B', 34: 'E', 35: 'D', 36: 'G', 37: 'F', 38: 'I', 39: 'H', 40: 'K', 41: 'J', 42: 'M', 43: 'L', 44: 'O', 45: 'N', 46: 'Q', 47: 'P', 48: 'S', 49: 'R', 50: 'U', 51: 'T', 52: 'W', 53: 'V', 54: 'Y', 55: 'X', 56: '[', 57: ']', 58: 'a', 59: 'c', 60: 'b', 61: 'e', 62: 'd', 63: 'g', 64: 'f', 65: 'i', 66: 'h', 67: 'k', 68: 'j', 69: 'm', 70: 'l', 71: 'o', 72: 'n', 73: 'q', 74: 'p', 75: 's', 76: 'r', 77: 'u', 78: 't', 79: 'w', 80: 'v', 81: 'y', 82: 'x', 83: 'z'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = {ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "print(char_to_ix)\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print vector_for_char_a.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "seq_len = 25\n",
    "learning_rate = 1e-1\n",
    "\n",
    "Whx = np.random.randn(hidden_size,vocab_size) * 0.01\n",
    "Whh = np.random.randn(hidden_size,hidden_size) * 0.01\n",
    "Why = np.random.randn(vocab_size,hidden_size) * 0.01\n",
    "\n",
    "bh = np.zeros((hidden_size,1))\n",
    "by = np.zeros((vocab_size,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs :  [51, 66, 61, 4, 47, 76, 71, 68, 61, 59, 78, 4, 36, 77, 78, 61, 72, 60, 61, 76, 63, 4, 34, 33, 71]\n",
      "targets :  [66, 61, 4, 47, 76, 71, 68, 61, 59, 78, 4, 36, 77, 78, 61, 72, 60, 61, 76, 63, 4, 34, 33, 71, 71]\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_len]]\n",
    "print \"inputs : \",inputs\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_len+1]]\n",
    "print \"targets : \",targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs,targets,hprev):\n",
    "    xs,hs,ys,ps = {},{},{},{}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    for t in xrange(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1))\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Whx,xs[t]) + np.dot(Whh,hs[t-1]) + bh)\n",
    "        ys[t] = np.dot(Why,hs[t]) + by\n",
    "        ps[t] = np.exp(ys[t])/np.sum(np.exp(ys[t]))\n",
    "        loss += -np.log(ps[t][targets[t],0])\n",
    "        \n",
    "    dWhx,dWhh,dWhy = np.zeros_like(Whx),np.zeros_like(Whh),np.zeros_like(Why)\n",
    "    dbh,dby = np.zeros_like(bh),np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(xrange(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        #derivative of output bias\n",
    "        dby += dy\n",
    "        #backpropagate!\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "        dbh += dhraw #derivative of hidden bias\n",
    "        dWhx += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "        dhnext = np.dot(Whh.T, dhraw) \n",
    "        \n",
    "    for dparam in [dWhx, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "    return loss, dWhx, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " .E8.f)p.,7h$fz U?S6LRW1\"bC�ty/�U)gM.Q$p/z:c723KOncDp$:'LJ8A\n",
      "I4dK\"OFIGUK@dqz?d/G@!*k$#2LIiNy%QFbSkpHj'h!Y7vMY:T:ccM4H'R4tg]T@PB7s)i6(-HW\"Lcj[tgDV6f!�Wmul9d*lcJhV F!),VePjufS ;s\n",
      "Wvj8a9pTLB40cs9-NI8P,4G0 \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"                                                                                                                                                                                         \n",
    "    sample a sequence of integers from the model                                                                                                                                                \n",
    "    h is memory state, seed_ix is seed letter for first time step   \n",
    "    n is how many characters to predict\n",
    "    \"\"\"\n",
    "    #create vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    #customize it for our seed char\n",
    "    x[seed_ix] = 1\n",
    "    #list to store generated chars\n",
    "    ixes = []\n",
    "    #for as many characters as we want to generate\n",
    "    for t in xrange(n):\n",
    "          #a hidden state at a given time step is a function \n",
    "          #of the input at the same time step modified by a weight matrix \n",
    "          #added to the hidden state of the previous time step \n",
    "          #multiplied by its own hidden state to hidden state matrix.\n",
    "          h = np.tanh(np.dot(Whx, x) + np.dot(Whh, h) + bh)\n",
    "          #compute output (unnormalised)\n",
    "          y = np.dot(Why, h) + by\n",
    "          ## probabilities for next chars\n",
    "          p = np.exp(y) / np.sum(np.exp(y))\n",
    "          #pick one with the highest probability \n",
    "          ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "          #create a vector\n",
    "          x = np.zeros((vocab_size, 1))\n",
    "          #customize it for the predicted char\n",
    "          x[ix] = 1\n",
    "          #add it to the list\n",
    "          ixes.append(ix)\n",
    "      \n",
    "    txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "    print('----\\n %s \\n----' % txt)\n",
    "    \n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "#predict the 200 next characters given 'a'\n",
    "sample(hprev,char_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 110.770425\n",
      "----\n",
      " JYV:RID�IC7GGyT!ji k*ffxtQJGq\n",
      "\n",
      ".cUHd!lW,%ih3mB6sk/IP;c,![-RL/KwW8C7T:o3:XQCKeNuRk�.Rg1'AtAWV[�dBj[)TqN.kcA'pydj\n",
      "r!/w$RHX\n",
      "PTx]@\n",
      "W'U@XiF9#))u*SkHtkg7@11tz,'@Xu:Jh#R/Q7Y\"'t('$B�nN)S@YECjL*D'VjgAHt@�.;RaJ \n",
      "----\n",
      "iter 1000, loss: 88.075559\n",
      "----\n",
      " nlMre eeh tot aaeeesvd be aig fh n ' he5 ba .oBsmnes bi sqaTHyo tess hers oene f mart ti'yhir apahirrtyr cifla t to bo ti te the cailhew ono me knton piuy s the thr h oo ns bsr daa Gcr thr (os ng ceso \n",
      "----\n",
      "iter 2000, loss: 71.398329\n",
      "----\n",
      " ha dor lot weulyot. ghito ane s bas sto ky aocr in to hee khar ait bet his se\n",
      "riG Gut tht sut hao They meuIe; dfe nder hit afe shy th hisetlant lhe sitben anstee waathed tout, me. whe hurem the\" Wt sl \n",
      "----\n",
      "iter 3000, loss: 62.543242\n",
      "----\n",
      " to tof nod cim ato to he acthecligdf dh in tors tactir baer dhe bot ame bwsrene, gcant has,,, Gcre he cay bmw; qulcm, yor hed des fiid jvannard sion s s;es ind fo hain sal ald oond ant ano tcotishat t \n",
      "----\n",
      "iter 4000, loss: 58.484029\n",
      "----\n",
      " gheme cart wased tons his frhef sh coupld the mocad tiy uf so, rereer f thad Gren to deto her ebon ne, tals therof cuke indes atigy ongCenoathe roweugasBn with the thech. hakely cuyke t tor tot r,,ord \n",
      "----\n",
      "iter 5000, loss: 60.267632\n",
      "----\n",
      " iind utf thed OCulacsfry;w bIw Jey kin qren Iud worow, ixn bablerastan ont sur'nucsugowl'ts eumenl themor \n",
      "eathiid bes tottip's haf he Gregm rrecyom coonm\n",
      "he sowttighid thentecwis\n",
      "utiky andosoki1.\n",
      "riy \n",
      "----\n",
      "iter 6000, loss: 62.430964\n",
      "----\n",
      " wd hime gobe fosead brentond pss out sded nensr whee baveleles hard\n",
      ",s uredulcresrar he the ule hopd an bot oudmaest tf aane hal wincirm -ne stivet becl un ard cathat wasreris sougercerlosseed ant lam \n",
      "----\n",
      "iter 7000, loss: 58.015009\n",
      "----\n",
      " ot abcausyo hith he veor dot halith mofrer nos ofws nlund freas noth fit af tos la n to booct ef pon ho felcer br;o mecther fok.!'it eroby mosiyghe refellibghe frers..,\n",
      "!Has'sr a thalgar goring carlig \n",
      "----\n",
      "iter 8000, loss: 54.559421\n",
      "----\n",
      " us sout weec nhe beghell thame mefucbe hipit he pood nhist goull 'lt do the chinke coreis aplemt fowingitly bumsatiif tore thersruby sromlr sthere s and wo reg duk peen onosk inky shis aitorutereek th \n",
      "----\n",
      "iter 9000, loss: 53.214922\n",
      "----\n",
      " o her weanles,ese way is, spas would no hieg nom tas stengat, but bped to sisp ace chen\n",
      " beecen falley te fith the sencower has had andirke bed wenout ut ard tha lerefpath her terr ore ax nesy of thow \n",
      "----\n",
      "iter 10000, loss: 52.407387\n",
      "----\n",
      " g ereanded.\n",
      "\n",
      "\n",
      "9 sald orrovarl ook on hele whatdeghy ceft ale, ht wat Itom heedingey hull, hit the \"hy courde oon 't dustorferele of baper fanchem thorket allely, peanst aghe thiny ther, wad. Shad to r \n",
      "----\n",
      "iter 11000, loss: 59.504649\n",
      "----\n",
      " if ched ands amppmatouby te promer ank to pate that Gc7aste sion of enimcilepoucirr of roje tor mome. andecitey entarn as gos eve hilarer bars\n",
      "it pinged to oreint. agqur windem aldm isp fror apall. sh \n",
      "----\n",
      "iter 12000, loss: 56.674394\n",
      "----\n",
      " hick ave itr bet cion hal\n",
      ", ane,s tomalgr in becon bope the noy he a these wiig waf be it Gre lay low me has; stee atingre hepsed frour fovex alhe slow pakiny, cumente wo the s amselre in was G, s ono \n",
      "----\n",
      "iter 13000, loss: 53.251875\n",
      "----\n",
      "  lad sewery, his werebysit; siled hes hiscdacsoranledly in this the papeay oun the \"ace ofk core wowe hey toughed beld thole lromter Gregould feer atsor sha dowed.\n",
      "\n",
      " beed ha codM sout's hindey yore ht \n",
      "----\n",
      "iter 14000, loss: 51.199316\n",
      "----\n",
      " e wioms elifl/f icwitheer fupthe ther shire tasy so in thacken cothed fott ofr'bgathed solen the Onere of mettenst she congd chllinger noer, amsenno the futhe doullka Mrighing thta aid nady belbabe na \n",
      "----\n",
      "iter 15000, loss: 50.377155\n",
      "----\n",
      " enw, the ploro verolede fis rarpegorsack as ot aus enttoun shere, tothe lgos ruwhey quot gore tust ofe to hero1 tha dey wimsidy on't in bust the thakn thesinf dlas, spapen, his ine the the noradedy to \n",
      "----\n",
      "iter 16000, loss: 53.032477\n",
      "----\n",
      "  efeit nos\n",
      "\", out or rock ca ap to weatirely who luste? noo deabe\n",
      "ris piss and mesed hat rout and puster acithe abpen, the rar'dpat histet thain\" the Grenttey; a medry io\n",
      "fel hsm. wo whising and poule \n",
      "----\n",
      "iter 17000, loss: 56.841830\n",
      "----\n",
      "  indprace ti rees cais of neet a the wercuen he of then abug cow qmin core the thoun, nonan tero Bis Grest\n",
      "5Xer iud wates prayed hiched wactly pom. Se wes of entuing allociledy. He he, sain the ie anf \n",
      "----\n",
      "iter 18000, loss: 53.477005\n",
      "----\n",
      " mund o vis whik it tercinas and erevet an he might wh sowe wht at tlebf hand; to dojeckim, ha dott, Hot fing traant forking ut onot aulans. in sious thit momet the hidd elery woy hus. And anking, the  \n",
      "----\n",
      "iter 19000, loss: 50.692736\n",
      "----\n",
      " af le all eroos. Bup inger him rot tamr sishyont sout wouthad roment, rebentide he wikinse doict. chandy othe ull the onn. Fas watthat fome folle tsolen at; to fuefle  onecbicly pteafr and bem coule c \n",
      "----\n",
      "iter 20000, loss: 49.839019\n",
      "----\n",
      " bit la bior Pher and forSl bioe dof nighe thor to fop, cald in b af clalto neting on? Greg hit of and in ano lpithed cutheprnaterit rowheme pacpfro he mart thord or her now on fre had laully bubked th \n",
      "----\n",
      "iter 21000, loss: 49.326710\n",
      "----\n",
      "  in hinelntetrart have thandly had eregor thim uf and coth herk lates ir's ker. Hoth co liin led hady has the fish tlasly, bey \"ur him he.ved could eaplis sime it in sofes wo he tieg toon was the leav \n",
      "----\n",
      "iter 22000, loss: 54.782778\n",
      "----\n",
      " on that Pnoninc, in or parg-or bare,, he\n",
      "ngwader Grysh Groct thasltes nomie heve sther uss f or int or to stousetect bus, thory lo mly oripupmencanise llinr afdentutbe Thr\",. dind Muct the nfive dion  \n",
      "----\n",
      "iter 23000, loss: 53.861816\n",
      "----\n",
      "  his hiin af enwerer lis in srid reaveen in it rid se the nis esijoG ce. both bot mam tuimesing his larle wis mits pus ingy has disdle lee tha sey aly his. Tonting.\n",
      "/ Foot\", his bey uly.\n",
      "Fur TCI*66 he \n",
      "----\n",
      "iter 24000, loss: 50.849918\n",
      "----\n",
      " dower sea dre bace her. He whis in ted is way toved puritge fitt achash it at croon borPrectord of thate proce nouster pustpr hotht he wothed plengad whilly cldmligf and the thepe, gong tour nated hir \n",
      "----\n",
      "iter 25000, loss: 48.945091\n",
      "----\n",
      " ot poowt liter beernt oor winly and efther ti fimed was whor thes oked therend norly, \", not wimy the p(of to coded csbouce, leace coy th huud werlaling glexy steruint whe sut door the lers aply, somi \n",
      "----\n",
      "iter 26000, loss: 48.356832\n",
      "----\n",
      " Gne is elifper msy\" the sistone, that cheather cay the wiy't. She Greghigho he ther at Prego watherking. I thew, gould hhoven a eay hit, wholly ane had and nish tamed, ctilf, it Gregor the wert nom re \n",
      "----\n",
      "iter 27000, loss: 50.225004\n",
      "----\n",
      " i\n",
      "m histes rereable induldo. Ma thers, clogbicc, I�w. Shisn it in go whiom and aldeather aawh, mod, lodestaning not's dey of abe suther docmate would to difr he is revery lartuit she last. TThe sAand  \n",
      "----\n",
      "iter 28000, loss: 54.946148\n",
      "----\n",
      " ghad ureel; to Faguces awsiinge, shone of his woulims everimeddenip) Gutecu Prodacre hiddels nut xrey of for eloncius wack So hadnsicl ip hack abuter elo whas ltat ag deepf all reansing whit dad end m \n",
      "----\n",
      "iter 29000, loss: 52.061394\n",
      "----\n",
      " ichion woubdout was wime al. Win. mh ryatsed fall the stuire the arefim out, adamprouacd, bot it hslea,, ops \"nour bucke smis rack to the'l, full frsiwe levenis the doure tout weshed hore tay him soly \n",
      "----\n",
      "iter 30000, loss: 49.419231\n",
      "----\n",
      " d not mels that regoje, itnyo thom lay ynoind the momes hid he sano bechelr for ir he csore eed mort in wass dithing of ttrilly mother to fate whinchind his monhore She wone shely in herre becer ho th \n",
      "----\n",
      "iter 31000, loss: 48.500812\n",
      "----\n",
      " he, ang she reingly. She lid of his wom oute'nd ole his tored aid tomproust ond the; vasnt woos dit, harres wifrod on aven of the foficpabl indt him, wam wime to wat ably thut was ast hur aldete shle  \n",
      "----\n",
      "iter 32000, loss: 48.263206\n",
      "----\n",
      " sey maed waif nourtusemershe here his shes his. His ar wouw tuontar and out mhe as to tut himing ding thel not aras his niting the'f? BI unde simither othe romess thut thow ald eftheint it eitu him it \n",
      "----\n",
      "iter 33000, loss: 53.258053\n",
      "----\n",
      " yediining oven\n",
      "y toution's fereing - fr osed itrrotm lems undut\n",
      "9Fce wook on with ushigyce or iiclute? Yx\n",
      "IC4,\n",
      "AD HE IH TLSo(ut beed becc was Gr.V, deaded. Hinr e vit aceibling this wozer sha\n",
      "liver fo \n",
      "----\n",
      "iter 34000, loss: 52.512131\n",
      "----\n",
      "  wide ott to has of d tully The sanneed an him soveabe bas; by could creas\n",
      " of tea dound every. The fuis lablist everencirnen dijventh. Ro the ray mack, shay rieg. To mof in in couked af jucing co her \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 35000, loss: 49.721476\n",
      "----\n",
      "  to lay on  thicp to lidnout, pough ther was sovede alder wumor in, llyo; wert the Prook tavere had 1rojedmabepere, be- ag. I way yo and to roemt it ind a periddadltt. ste tovey in stor's but the hmig \n",
      "----\n",
      "iter 36000, loss: 47.899969\n",
      "----\n",
      "  this the sheeto hid a beremer his tally he able balling no the ind have wio hist the was he wass a sepien is sike lpesm at his to forilyst one beCT rustral pre ond ther; bit, ared shate fald extert u \n",
      "----\n",
      "iter 37000, loss: 47.290892\n",
      "----\n",
      " erby that ereghing hen t in in thes as, qumer the weak it eos to the not old to firge waley the dimrer fading agow ho dord carden that anduce, the wos lay uis hus far. Af bke stuse, ofe work us andnea \n",
      "----\n",
      "iter 38000, loss: 47.956543\n",
      "----\n",
      " akfion thive ked, overning deert whay it ast- pooced moom st efat sismen aid eroTo gothing his nemilod ounsaw sist waspquyitied all, sisss\"eds i'd had sase monoughed oulter to chas dotily experting he \n",
      "----\n",
      "iter 39000, loss: 54.017155\n",
      "----\n",
      " hed Inbht the Purneg tos fsore was that thentennekestouth s th in of the had wars and gound tliofl works and it to and lokenr proppert, asastite cotilesn. pibentete him it oll or\n",
      "wat of cate it whime  \n",
      "----\n",
      "iter 40000, loss: 51.111306\n",
      "----\n",
      " o tt. Int Itiaxe ccond the sidteasand ro dape tlempighpig aw the hery word reabeld but. in noc wivisted ngered the uldypuse Grrect'th morel. N r of pice inred neting to yatering andelcer to theed bas  \n",
      "----\n",
      "iter 41000, loss: 48.394986\n",
      "----\n",
      "  was wath had fordid whlhed. She toor hold hord histion abenterel\n",
      "that erelprernon hos had appishel save as he roolr Prons -he lent the hey - and this reitving comea (ededsauned asc thought yegh the h \n",
      "----\n",
      "iter 42000, loss: 47.435811\n",
      "----\n",
      " ly ackine er. a unswas in agon \n",
      "Gr-excouker in the saeem. I wincorgln wouched glataking terke a batexlyowsunbed the pease dot, ping tor ppleridstmy woamive thigl wound simpinf the yo eded 5o havice ch \n",
      "----\n",
      "iter 43000, loss: 47.195772\n",
      "----\n",
      " tonten mindub, a 5ite his bdort yousde and ites\" coor woumthed led, ant time and thay marye tontrermutser the ceant pusoly wass, whay ito coubturent was haccef, ain sevpatiag if a ditardent Gregorubor \n",
      "----\n",
      "iter 44000, loss: 50.753084\n",
      "----\n",
      " got, kisturt bege Recems bes coullisth Pregon yeed ySeres, enoniving muther; Arls ast selitise; of the gee and ass pret. Thas sinylo bxcoutieg therly, ghis cows crasemy, vet, rooms plic recownt tho go \n",
      "----\n",
      "iter 45000, loss: 51.462581\n",
      "----\n",
      " bing.\n",
      "IEx(\n",
      "1.E\" Duck work It by etoel this a distith \"He to roon. Thew det Greny ind wouge interttly.\n",
      "\n",
      "T to Projead, Grego chs exttrbow of itter the dowe whond the shat ling herk Greger wichous moun h \n",
      "----\n",
      "iter 46000, loss: 48.914374\n",
      "----\n",
      " uribke rungAnt on'cfave wamme sthusething, backwir fere sir and utsittedmang to not the himmen pajughe here fada wand wite tine all tuokmon rught  cat brays ould carded it, aed as no he wer. Wounder s \n",
      "----\n",
      "iter 47000, loss: 47.118513\n",
      "----\n",
      " sceinn and doplund rove ald then allcleonhing thit coped tor she cpremseld porsitheld, the the eve fher was not Gregor to cher in, the of thith, ded shou not alpfored st, noud the camass oug ssome, a  \n",
      "----\n",
      "iter 48000, loss: 46.477371\n",
      "----\n",
      " f tomath, ang it thoo it as notwing her. Horg buss he fley. Alsinnuw or!\" hirly Projerit and his efo the diy utw evenr, that he any wally sherinis ton, foring noen tubke Grecilly dot rorequt told of l \n",
      "----\n",
      "iter 49000, loss: 46.996872\n",
      "----\n",
      "  the and him s lay the centd anduw an, at ding ano go darsess lacion antor mocking gle. But tementen on table osed bfet spiter the roon bnour whenn qppone all. The wiok shee ther tood ynot hesars and  \n",
      "----\n",
      "iter 50000, loss: 52.717964\n",
      "----\n",
      " Neche als agis weres abl and ve his her tedaste a dint affely ant withay ousa fouly tor Hem sone. Satay, allome the cerm puaven ly stithen. He pally hemser case ant wow. Pregare targ- pro1l\n",
      "\n",
      "Founly a  \n",
      "----\n",
      "iter 51000, loss: 49.908360\n",
      "----\n",
      " ey of puss, ner calliss asker was ly ontiod shentiforg to his roours with the sla njer@Shick ost thot, wall out blentilg, tems he fall brid, allered the key. \"Pembinbete the anven wele bone int was of \n",
      "----\n",
      "iter 52000, loss: 47.568977\n",
      "----\n",
      " hing themy\n",
      "ree his mar alo hard then's iss.  theiche bich as was strotered that wondy of lsye cs lcoi ans; this timeacl. GrEgat, at, ver diit sis to the Prevares ins chay Grecourd soundly shey rouar f \n",
      "----\n",
      "iter 53000, loss: 46.521380\n",
      "----\n",
      "  ited couiqut anenbeverg. -uat of and, if for a perses enos entigrese stiake up ate, any ho laan out ang hit her lother to whe sim, wat even; Gut blig thit vion tore ur well naring; hive for wable aa  \n",
      "----\n",
      "iter 54000, loss: 46.118550\n",
      "----\n",
      " d and whressone formprooked theic he reakec Gregowheky it womencildy whentever mably dick ould weiked neamself, by oth ristion theinfe momitteter thin mom herk hinmsoright was yatser bling thee. Om Gr \n",
      "----\n",
      "iter 55000, loss: 49.115585\n",
      "----\n",
      " ughe the ikered to then Gregou he watirg-the ary in inc prowsan.\n",
      "1, the piass nt to chade, oriemall\n",
      "iy rought way wourur Gugot chat for a deture frow melyfat Guten, if thoo ar - enchres.e - wore  frab \n",
      "----\n",
      "iter 56000, loss: 50.506450\n",
      "----\n",
      " ras buco inporm, doute, could co oler a mfa or ap bros by the had bee loom ande to like collto ot mave he sick atres seabed Gregor's he that tiomr onlyer for what trottround but an dracken id to placl \n",
      "----\n",
      "iter 57000, loss: 48.278570\n",
      "----\n",
      " ct he sear ing, of alround the now any tome dowt ptren forkserdesn ar he. - no\" Ting the belled wo wloly he that ghing bestustet, a carefwiot inte his was had bpsiy she himomter froner that ing forr,  \n",
      "----\n",
      "iter 58000, loss: 46.304721\n",
      "----\n",
      " dise his pashemely shamer dool shot lfo distorthen axa was it he touce to mores and ha mich horgigh, chand.\n",
      "\n",
      "Itsed tiore theed eavion Grecksie. The sloust ma ca alancom buced freen their whinf ware, w \n",
      "----\n",
      "iter 59000, loss: 45.750790\n",
      "----\n",
      " s conly wean rey ting sted his un wishea pose of thy whillst not, had derowp ever he wout claetistent, Gregor!, the cous whimed, out wothf and could soness, dider, could. It on, pardeas som was forent \n",
      "----\n",
      "iter 60000, loss: 46.195105\n",
      "----\n",
      " Of the rultily ing, fee fling on his woull noon to ade clackest. Bus. his sameatise the coudy of ave takile Gregor browfch proth, aws sheer, yot fithed fens was bems had dity they at and sevary \"I his \n",
      "----\n",
      "iter 61000, loss: 51.731879\n",
      "----\n",
      "  as toure not Cuth cohmerd whe of the fra wouts to e)unt teardethemen.\n",
      "\n",
      "\n",
      "TI't anl slouthr-m remlyt\n",
      "sed nom of sille wone ueres ant reeresaricessed in ano in, the stromad, doo, focrilis coully ive cout \n",
      "----\n",
      "iter 62000, loss: 49.236777\n",
      "----\n",
      " s nor agate had nebable hid st ow wever whersed and thoug? Bberg af ing to suader axpeth. I*\"\n",
      "\n",
      "Fame it was lede comapfporhigft inid ceed it its wootur shey swuynouction qugten. He eathed had not exp o \n",
      "----\n",
      "iter 63000, loss: 46.946431\n",
      "----\n",
      "  father ancever taid supap go wive hed hist the roordy thit had ofher stoon!o, chathuntorg kay for he wisated shough he. His inls withen ever, sext lam the knet she cherest, he the roiccail it in they \n",
      "----\n",
      "iter 64000, loss: 45.816143\n",
      "----\n",
      " stcawink sirpture staved tarn. He to mishes of no clis hicnared soutrater seat lisurennerss wad on a vooked that ie; th2oricppeon elet and on himstintho it. He nofr his entern could ate and could dow  \n",
      "----\n",
      "iter 65000, loss: 45.404693\n",
      "----\n",
      " d njors lofay himey the inen was sarely, wit be jusy ackeghtingy shim the case had alate way of sough, thad be bay sape could pick to in alf, virlive. eney nortorno soo therak that in regocl. As to'st \n",
      "----\n",
      "iter 66000, loss: 47.790964\n",
      "----\n",
      " e abrouglisadlyouss oull gat sobecte praym rerendoughigs musm, coulo wit wink of dis frectair ansavy be wout oser lfas hing mentts fund.\n",
      "\n",
      "Gregor beet kely nicks ade, anreest clinkirm rosiels. -ue od r \n",
      "----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-d953b785251e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-c87bb630e4f5>\u001b[0m in \u001b[0;36mlossFun\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWhx, mWhh, mWhy = np.zeros_like(Whx), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_len # loss at iteration 0                                                                                                                        \n",
    "while n<=1000*100:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    # check \"How to feed the loss function to see how this part works\n",
    "    if p+seq_len+1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data                                                                                                                                                             \n",
    "        \n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_len]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_len+1]]\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "    loss, dWhx, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    # sample from the model now and then                                                                                                                                                        \n",
    "    if n % 1000 == 0:\n",
    "        print 'iter %d, loss: %f' % (n, smooth_loss) # print progress\n",
    "        sample(hprev, inputs[0], 200)\n",
    "\n",
    "    # perform parameter update with Adagrad                                                                                                                                                     \n",
    "    for param, dparam, mem in zip([Whx, Whh, Why, bh, by],\n",
    "                                [dWhx, dWhh, dWhy, dbh, dby],\n",
    "                                [mWhx, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "    p += seq_len # move data pointer                                                                                                                                                         \n",
    "    n += 1 # iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-tensor-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
